# Distributed Systems

## Introduction 
A Set of cooperative computers working together to get something done.
- if a single computer can do it, plesae do it.
- try everything else before going to distributed systems.

- Achieve Parallelism
- Fault Tolerance (if one of them failes, we go over to other one)
- Physical Reasons (Data centers spread across the globe)
- Split up the computations
- Security / isolated
We can have partial failures because network is included in the architecture.

## Basic Challenges
- Concurrency
- Partial failure
- Performance

Very important part of computer infrastructure.

We have lab assignments too.

## Components
- Lectures (Ideas on Distributed systems)
- Papers (Classic Papers used in Distributed Systems and solving these difficult problems)
Read the papers before attending lectures. Learn how to read papers. Skip parts we don't actually need to read.
- Exams 
- Labs
- Project (Optional)

## Labs
- MapReduce Lab (the paper we read)
- Raft for Fault Tolerance(Fault Tolerance. Manage automatic cut over)
- Build Fault Tolerance Key/Value Server
- Clone that server to individual replicated server. Sharded (Partitioning the data) key/value server.
- We can build our own project as well.


## Infrastructure
- Storage (We focus the most on) (Distributed storage)
- Communication (Used as a tool to build Distribution System. We are kinda consumer of communication.)
- Computation 


Build abstractions which hides the implimentation of these details. We would like to build an interface which looks and
acts like normal storage/communcation system but in actual it is Distributed system.
We would like to have Abstractions. Really hard to have achieve it though.

## Abstractions
- Implementation (Remote Procedure Calls (RPCs), Threads (Structuring concurrent operations), Concurrency controls 
(locks). All of these are important tools. We need these to build distributed systems.)


## Performance
- Scalability (Scalable Speedup. If we increase the number of computers, we have, our performance should increase 
. E.g. 2xcomputers -> 2xthroughput -> 2xPerformance. We can easily buy more computers, so having this kind of 
scalability is always desirable. The alternative is to change our program, have better algorithms, which will always
be very difficult to obtain. The second way is very expensive way to go.)

Users -> Web server <-> Database
When number of users are less, it is easy to have the above behavior. 
But when the number of users increase to millions, we need to increase the number of web servers. 
Now, we want to send some of our uses to WS1, and some of them to WS2, and since all of the users should be able to
see the same kind of data, all of our web servers needs to connect to the same databse.

As our number of users increases, our web servers will keep on increasing. This will give up a parallel speedup to our
performance as long as our web servers do not put a lot of load on DB.

At some point, when we have 100 web servers, now it puts a lot of load on DB, and now DB becomes the bottleneck.
Now, we need to make some more design work on Database to make it work with this parallelism.
Database design work is hard.

## Fault Tolerance
A single computer can stay up for years!
When we are building computers with 1000s of computers, even if we have computers which runs up to a year, there
is still a chance that 3 computers will break in a year.
Something else might also happen with something else, like network cable, electricity, etc. There will be constant 
problems. 
That is the reason we should always have a Fault Tolerance. Even if we have a high failure rate, our application should
be able to handle the situation.
- Availability (Under some kind of failures, system will keep operating)
- Recoverability (If something goes wrong, something will stop working. After the repair, our system should be able to 
 get going ahead with all correctness.). 
 
- Non volatile storage (hard drives, or flashes, to store a checkpoint or a log
 to store the state of the system. We then read the state from our hard drives, and start from where we stopped.). 
 Management of Non-Volatile storage is expensive.

- The other big tool we have are Replication. We have two servers, where one server is the exact copy of the other server.
Using replication, sometimes both of the servers might go out of sync. This again can cause a number of exceptions.
We will use replications a lot. This is a pretty complex topic.

## Consistency
let our example be a storage to store key value pair.
We have a put operation and get operation
Put(key, value) 
Get(key) -> value
Before we write this application, we should have a documentation of what these methods this.

Assume that we have a server and a replicated server
Server 1 -> (1, 20)
Server 2 -> (1, 20)
Now, a client sends in a put(1, 21) request to Server1 but while sending the sane request to server2, it crashes.
Now the data that we have is
Server 1 -> (1, 21)
Server 2 -> (1, 20)
Now, when a client sends a get(1) request, they can get 21 or 20 depending on who they talk to.
Fault Tolerance can make us talk to the server 1 always, but it might fail too. So, someday, we risk exposing our
client to stale data.
So, we need to write some rules down for the put and get methods. We need to have some good consistency.
Sometimes, it is also useful to create weakly consistent systems. These systems do not make these guarantees which are
good with consistencies.
- Strong Consistency (Guaranteed recent copy will be received. This is very expense as it requires a lot of 
communication to get the response.)
- Weak Consistency (Won't get recent copy. This is not that expensive. People will usually try to build these weak 
consistency systems.)

When storing our replicated copies of our servers, we would never want them to keep it in a single machine.
It is better to keep them away. Better to keep them in different cities if possible, so that if something goes wrong, 
both of our copies of server are not destroyed.
This is the reason we have multiple Data Centers across the world. But if we do that, we will require a significant time
to update the other copy. That makes Communication Required for Strong Consistency. 
If we have a put operation, we might have to wait for multiple milliseconds for all the replicas to be updated. This 
requires a lot of communication between servers which is every expensive and time consuming.
Because of this, people usually build weak consistency system, where they just care about one replica which has the
latest copy.
We have a lot of industrial and academic research on how we can use weak consistency for high performance.


