# GFS (Big Storage)
Google File System

Designing Storage System.

How to design a good Storage System.

## Why is it hard?
We need performance. This is obtained by Sharding.
Faults are everyday hour occurrences. We need automated fault tolerance
system.

Performance -> Sharding

Fault -> Tolerance

Tolerance -> Replication

Replication -> Inconsistency

Consistency -> Low Performance


## Strong Consistency
Clients C1, C2, ... sends lots of parallel requests to Server S.
This Server is updated the database.


Let's say we have clients
C1 (w = 1), C2, C3, .. C4(w=4) 
Now, c3 is reading w and c2 is also reading w.
What will their values be?
c1 and c4 wants to update value of w to 1 and 4 respectively.
This is dependent on which request server finishes first.
If c1 request is finished first, and c4s later, we will get value as 4.
Else, we will get value as 1.

## Bad Replication Design
2 servers, S1 and S2, each with complete copy of data.
We hope to keep these tables identical.
Every write must be completed by both servers.
Read to be done by either of server.

Let's say, C1 wants w = 1, C2 wants w = 2. These writes will happen
to both of the servers.
What is going to happen here?
If S1 receives c1 first and c2 later, it's w value will be stored as 2.
at the same time, if S2 receives c2 first and c1 later, it will
store value of 2 as 1.
S1 and S2 now have two different values which is bad.

Now, when C3 and C4 reads from S1 and S2, both will give us 
different values which is bad.

This can be fixed with more communication between servers.
There are different solutions for these kind of scenarios.


## GFS
GFS is fixing the above model that we discussed.
Came in 2003.
Giant youtube videos, all of web stored in database, and all other 
data was stored in the database. 

What they needed was Big and Fast.
Any application should get at it.

Global Usable Storage System.
Anyone inside google could read this File System.

Sharding -> Every data will be automtically split in different parts.

Automatic Failure Recovery -> Fault Tolerance

Single Data Center -> GFS was supposed to live in one big data center.

GFS was for internal use. They were only used by Google Engineers.

Big Sequencial Access - Tailored for  big sequencial file to be split
Terabytes of data.

Use of a single master without fault tolerance and shit.
One master just worked fine.

## Architecture
c1, c2, ... n number of clients
Single Master Server.
We have 100s of Chunk Servers which has their own file systems.
Master server knows where each file's split (chunk) is 
stored in Chunk Servers.

## Master Data
Two main tables that we care about.

Non Volatile - Written to Disk

File name -> Array of chunk Identifiers (Written to Disk)

Handle 
- List of chunk servers that holds replicas (Not written)
- Version number of each chunk (Written) 
- Primary chunk Server (Not written)
- Lease expiration time of primary chunk server (Not Written)

This data is stored in memory as well has disk.

LOG and checkpoints -> Written on Disk

Why use LOG? -> We can append to a log every efficiently.
It makes a little bit faster. 
Checkpoint when the log was created is also stored on the disk.


## Read Operation
When Client tries to get a file name
1. Application sends File Name, Offset to Master
2. Master sends Chunk Handle and List of Servers to client
3. Client caches the chunk handle info
4. Client talks to Chunk Servers
5. Chunk Servers Return the data to the client


## Write Operation
When client tries to write a file

### No Primary Chunk Server?
On Master Server
1. Master Server tries to find up to date Replicas in all chunk servers.
Version number is used for this. It will check if version number
present in master is available in the chunk server.
2. Master then pick one of them to be the primary and others 
to be secondary servers
3. Master Increments version number and writes that to disk
4. Master Tells Primary and Secondary the current version that 
they hold
5. Master writes it's own version number to disk after telling 
to Primaries.

Now we have our primaries.
6. Master tells the client who primaries and secondaries are.

### Has Primary Chunk Server?
7. Primary picks an offset and tell all Replicas
to write at that offset
8. If all replicas sends the ack, then primary replicas success 
to the client
9. If something goes wrong, Primary replicas NO to the client.

if clients gets a no, he has to redo the operation.

Does the lease expiration change the primary?
Only when primary is dead.

SPLIT BRAIN - Caused by network partition. Some network error when
master cannot talk to primary but primary can talk to clients and 
secondaries.
In order to avoid two primaries being assigned, we use lease.
Now, when lease expires, master can only assign a new primary only 
when the lease is expired. This way, we can avoid Split Brain.


## To make strictly consistent System
Detect duplicate requests in Primary?
Make sure Secondary accepts Primary requests without fail.
Have multiple phases in writes? Making sure that writing is 
definitely possible. This is called 2 phase commits.

When selecting a new Primary from the secondary, the new Primary has
to resynchronize with all the secondaries to make sure that data 
remains same in all replicas.

Have lease system for Secondary just like for Primary?


## Success GFS
Big Table build as a layer on top of GFS.
Same goes for map reduce, it was build on GFS as well.

The big con on this system was only one master was here.
It ran out of CPUs and memories because of thousands of clients 
trying to connect to master.

Not automatic story to restart master. Manual intervention was 
required.

