# Map Reduce
Originally used by Google.
The map and reduce is basically the sort of the entire internet. This is just like running a sort of the entire websites
available in the internet.
Sort is expensive. If data is very huge (in Terabytes), it can take weeks to complete.
So, Google had to come up with a technique to sort or analyse all the websites so that we don't have to wait for weeks
for job completions.

Looking for a Frame work to run giant distributed computations.
Application designers should be able to write just a simple map function and reduce function, and then the
map-reduce framework should be able to take care of all the ugly implementation details of distributed computations.

Map-Reduce framework expects some input files
Input1 -> Map -> a-1 b-1
Input2 -> Map ->     b-1
Input3 -> Map -> a-1     c-1
Reduce(a-1, a-1) -> a-2
Reduce(b-1, b-1) -> b-2
Reduce(c-1)      -> c-1
... 
these files can contain many web pages, or some other information
Map function is run on these Input data. This map function should be able to produce a list. 
E.g. count the number of time a words are present in Input1. So here, word will be the key and count will be the value 
stored in the list returned by Map function. The map function will just output 1 for every word it encounters. If 
it encounters 'the' word 10 times, <the, 1> will be repeated 10 times in the list returned by map.
Now, we have our intermediate stage, where we ran our map, and we have returned lists which stores some key-value
information.
In the second stage, we run the reduce function.
Now, Map-reduce function will collect all the 'a's output and send them all to Reduce function.
Then it will collect all 'b's and send a different call to Reduce. 
This Reduce function will we called for every key which was generated by the map output.
The Reduce function is just going to count the number of occurances of the key.

This whole computation is called a job.
Any one computation of Map function or Reduce function is called a Task. 

Function:
Map(key, value) {key is the file name, V is the file content)
We split V in to words, 
then for each word w,
    emit(w, "1")

Function:
Reduce(key, list<k,v>) {key will be filename, and second input is list)
    emit(k, len(list))

Once the output of Reduce function is obtained, it can be again fed to Map-Reduce function for a different analysis.
This is called an iterative algorithm.
We can have big pipelines to do this kind of things.

Where does the map runs? -> it runs on the 1000s of workers (machines), and it has a master server which is organizing
all these workers. We have read it in the paper of how master is responsible for running all the functions on workers.

Map() functions produces files on Map worker local disk. Now, at the end, each worker machine has files in it's local
system which is the output of map function.

Now, map workers will send this data to reduce worker. Typically, reduce worker will need output from all map workers.
So, before we can even run 1 reduce function, it is very important to run all Map functions first. Now master will
talk to all worker nodes to make this possible. 
Reduce will write it's own output to a file.
GFS (Google File System. This is a cluster file system. It automatically splits up a big file into small chunks of files
64MB files, and distribute it across the cluster. This is just what we need.)

During that time, the only constraint they had was network. Communication between each machine had to pass through 
one central server, which bottle-necked the performance.
To avoid this problem, Google ran these map reduces jobs in all of these GFS machines.
So, the input files were kinda read locally because they were already present in the machine.

Now, when we come to reduce function, we need all of the values associated with the single key. These outputs are 
present on all the worker machines where map function ran. To get this output data, network is required.
This network usage was the expensive part of the job because this is a very vast amount of data.

Modern data centers has far more data centers. It has a lot of root switches now, where each machine can pass the 
request to any root machine.

