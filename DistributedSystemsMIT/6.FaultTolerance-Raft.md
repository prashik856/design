# 6 Fault Tolerance - Raft

## Split Brain
A secondary cannot be primary by themselves without consulting the 
primary server.
This can cause to data in-consistency when clients tries to talk
to primary.

If we wait for both, we are not fault tolerant, but if don't wait,
we are incorrect.

If our network cannot fail, that means that we make replicated systems
that didn't suffer from Split Brain.


## Build Automated Replicated Systems - Majority Vote
First, we need to have odd number of systems. 
This way we won't have symmetry.
e.g. 2 of 3 servers requires to be functional. This means we can 
survice the downfall of single server.

General aggregation
If we have 2f + 1 servers, we can tolerate f failures.

This 2f + 1 is called Paxos (VSR)


## Raft
At the top, we have a key-value server, and this application
will have a state (the values in tables).

Let's say we have 3 replicas running, which has the application code
running and Raft.

Let's say we also have some clients which makes some calls to our
application (some put or get requests). This client will just use
1 url to connect to application.

Now, when request comes to application, the request goes to raft and
then raft chitchat with each other.
Once raft has confirmed the commit, raft will then connected to appication
for confirmation. After this confirmation, the request will 
be executed by the application.

Once application executes the request, then it sends back the response
to client.

## Time Diagram
C1      S1(L)      S2      S3
1
        2           
                    3       3
        4(Replica raft response- Majority)
                    5       5(raft ack)
6(response back to client)

## Logs
Raft creates tons of logs. What are these logs doing?

The logs stores the order of operations which the leader performs.
The same orders needs to be performed the same by the replicas.

raft.go:
func start(command) -> returns (index, term)
// send command

applyCh, ApplyMsg -> {command, index}
All replicas will get this command and index.

## Leader Election
More efficient system if we have a leader.

### Election Time
If this time expires, we start an election

We increment the term, and in the new term we get elect a new request
When a new term start, we start a new Request Votes (n-1 calls)
We need at most 1 leader.
We need a yes vote from all replicas. Each server votes only once.
The majority of votes received becomes the new leader.

Every leader sends append entry to all servers. Every append entry
suppress new election time for each term.

If heartbeat time is t, our election time should be something 
like 3t or more.

The more max value election time we put, the more we are putting our
system in freezing state. This will harm our performance has client
will have to wait for a longer time to complete his requests.


## Contents of Logs
How does newly elected leader works?

Let's take some examples and look at certain situations and 
see if that can even occur?

S1: 3
S2: 3 3
S3: 3 3
The above situation can occur when S3 is the leader and while
sending the second package, it got lost when sending it to 
S1 server.

Let's look at another situation
S1: 3
S2: 3 3 4
S3: 3 3 5
Can this situation arrive?
It can. let's say for 3rd term, we have s2 as leader, the request 
came and it has value 4 in it's db and it crashes.
Then reelection occurs, s3 becomes the new leader, client sends
request and s3 updates 5 in it's state and it crashes.
This of course this could happen.

The command in 4,5 could not have been commited by the majority.
Thus, (4,5) can be dropped by the raft. It can either drop 4 or 5
because we need identical logs in all the servers.

