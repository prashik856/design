# Caching

One of the most important tools for scaling applications

## Why use caching?
Take data stored on disk, and we put them into memory
- Improve performance of application
- Save money long term.

## How does Caching work?

Client -> App Server <--> Cache <--> Databse

In between data server and application, we have our caching. 
If data is already present in the cache, the request doesn't go back to our database directly. This is very fast, as delivering data is much more efficient and faster.

## Speed and Performance
- Reading from memory is much faster than disk, 50-200x faster
- Can serve the same amount of traffic with fewer resources
- Pre-calculte and cache data in advance (Twitter does with their timeline. Precomputes our timeline in cache and then serve us very quickly. )
- Most apps have far more reads than writes, perfect for caching. 
(Twitter, a single written tweet will be read thousands or millions of times. We can just put this kind of tweet into cache which will help for better performance.)

## Speed and Performance IRL

Line(clients)        Bouncer(Authentication)         Club (Application, it has database)
----                    -                               ----------

Now, our bouncer can have the password and clients can tell the password directly to bouncer and he can just let them pass. 
this is pretty fast.
Other way, everytime clint gives password to bouncer, he needs to go back inside the club, ask it's frontdesk to recheck the password given by client, come back to the client to tell him if he can let him pass. This is very slow. Irl, this will piss off a lot of people. We need to do the same thing in our application. Whatever data which is being used a lot of time, can just sit in cache. Better way to build a system.

## Caching Layers
We can have multiple layers of caching in our infrastructure
- DNS
- CDN (Content Distribution network)
- Application
- Database


Client <-> CDN          -> App Layer <-> Cache -> Database (Database can have it's own cache)
       <-> DNS Server

Caching DNS while visiting a website.
Then we have content Distribution network. (Cloudfare). 
A lot of websites use CDN. When we want to server static photos and pictures, we usually use CDNs. Instead of going to database, we can push our content in CDNs.
Then our App layer cache. 
Database can also have it's own cache where it stores it's frequently accessed data.

So we have multiple layers all across.

## Caching Pseudocode - Retrieval
A simple case of case which can be used in our dictionary.
Redis or memcache. In the end, it is just a big dictionary which stores key value pairs.
```python
def app_request(tweet_id):
    cache = {}
    data = cache.get(tweet_id)

    if data:
        return data
    else:
        data = db_query(tweet_id)
        # set data in cache
        cache[tweet_id] = data
        return data
```

## Caching Pseudocode - Writing
```python
def app_update(tweet_id, data):
    cache = {}

    db_update(data)
    # update cache
    # We don't want stale data in our cache.
    # Also, we don't want our users to see the old version of our tweet.
    # So, the next request will read directly from database
    cache.pop(tweet_id)
```

## Distributed Cache
- Works same as traditional cache
- Has built-in functionality to replicate data, shard data across servers (If amount of data is big), and locate proper server for each key
We do this so that we can easily scale up our caching servers.
So that our system can be more reliable.

Distributed Cache
Active Cache            Passive Cache
Active cache will have it's replicas sitting inside Passive Cache. So if active cache goes down, passive cache can take up it's place.
Passive Cache (Backup) might seem a waste of resources, but if our active cache goes down, and we don't have our passive cache, we will have bigger problems because every one of our queries will hit database thus degrading our performance.
If 97% of our data is going through cache, our database will be overwhelmed when cache goes down.

## Cache Eviction
- Needed for 2 reason:
    Preventing stale data. We don't want something sitting in cache for too long. Don't want to serve our users outdated data.
    Caching only most valuable data to save resources and money. We might have a lot of data in database which will be very rarely requested. Doesn't really make sense to store that in cache.

## Time To Live (TTL)
- Set a time period before a cache entry is deleted. Data will be deleted automatically. Depends on how sensitive it is to keep our data frest. Longer cache for some blog. Short cache for tweet counts. Heart beat or some other case where we just want the live data. We don't every want to store that in cache.
TTL depends on what kind of data we are working with.
- Used to pervent stale data

## Least Recently Used (LRU) / Least Frequently Used (LFU)
- Least Recently Used
Once Cache is full, remove last accessed key and add new key
Whatever hasn't been accessed recently will be removed.

- Least Frequently used
Track number of times key is accessed
Drop least used when cache is full
Least frequently accessed will be removed.

Two different tweets, 
Recent tweet will be request millions of times, while an older tweet will not be request at all.

Facebook case study
Popular user updates a post, we dump that in cache.
If we have 100000s of request for that particular post, we might have a lot of data calls at a single time (when data was updated, it was removed from cache. Now all the recent calls are going to database.)
To prevent this, we implement something called as a Lease, which still serves as a backup cache serving the old data while the new cache updates in the background with the new data.
The scale really does matter. At large scale, these small problems becomes a huge problem which can crash our application.
These are the kind of problems which are never really thought by small scale applications.

## Caching Strategies
- Cache Aside - most common
- Read Through
- Write Through
- Write Back
